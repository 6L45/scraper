#!/usr/bin/env python3
import requests
import re
import os
import sys
import time
from urllib.parse import urljoin
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path

SLEEP=4
PARQUET=
PDF_DIR=

def sanitize_doi_for_filename(doi):
    """Nettoie le DOI pour l'utiliser comme nom de fichier"""
    # Remplacer / et : par _
    sanitized = doi.replace('/', '_').replace(':', '_')
    # S'assurer qu'on a l'extension .pdf
    if not sanitized.endswith('.pdf'):
        sanitized += '.pdf'
    return sanitized

def update_parquet_status(doi, parquet_path=PARQUET):
    """Met √† jour le statut PDF_on_S3 √† True pour un DOI donn√©"""
    try:
        # Lire le fichier Parquet
        df = pd.read_parquet(parquet_path)

        # Trouver l'index du DOI
        mask = df['DOI'] == doi

        if mask.any():
            # Mettre √† jour le statut
            df.loc[mask, 'PDF_on_S3'] = True

            # Sauvegarder le fichier Parquet
            df.to_parquet(parquet_path, index=False)

            # Compter combien de lignes ont √©t√© mises √† jour
            updated_count = mask.sum()
            print(f"  ‚úÖ Mis √† jour {updated_count} ligne(s) dans le Parquet pour DOI: {doi}")
            return True
        else:
            print(f"  ‚ö†Ô∏è  DOI non trouv√© dans le Parquet: {doi}")
            return False

    except Exception as e:
        print(f"  ‚ùå Erreur lors de la mise √† jour du Parquet: {e}")
        return False

def download_scihub_article(doi, output_dir=PDF_DIR):
    """T√©l√©charge un article depuis Sci-Hub avec le vrai lien PDF"""

    # Domaines √† essayer
    domains = [
            "sci-hub.st",
            "sci-hub.fr",
            "sci-hub.ru",
            "sci-hub.ee", 
            "sci-hub.shop",
            "sci-hub.wf"
            ]

    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3',
            }

    for domain in domains:
        try:
            print(f"\nüîç Essai avec {domain}...")
            url = f"https://{domain}/{doi}"

            # R√©cup√©rer la page
            response = requests.get(url, headers=headers, timeout=15)

            if response.status_code != 200:
                print(f"  ‚ùå Code HTTP {response.status_code}")
                continue

            print(f"  ‚úÖ Page charg√©e")

            # ANALYSE DU HTML POUR TROUVER LE VRAI PDF

            # 1. Chercher dans les √©l√©ments embed/object/iframe (le plus courant)
            embed_patterns = [
                    r'<embed[^>]+src=["\']([^"\']+\.pdf)["\']',
                    r'<object[^>]+data=["\']([^"\']+\.pdf)["\']',
                    r'<iframe[^>]+src=["\']([^"\']+\.pdf)["\']',
                    r'<embed[^>]+src=["\']([^"\']+)["\'][^>]*type=["\']application/pdf["\']',
                    ]

            pdf_url = None
            html_text = response.text

            for pattern in embed_patterns:
                match = re.search(pattern, html_text, re.IGNORECASE)
                if match:
                    pdf_url = match.group(1)
                    print(f"  üìÑ PDF trouv√© via embed/object/iframe: {pdf_url}")
                    break

            # 2. Si pas trouv√©, chercher les liens de t√©l√©chargement
            if not pdf_url:
                download_patterns = [
                        r'<a[^>]+href=["\'](/downloads/[^"\']+\.pdf)["\'][^>]*>',
                        r'<a[^>]+href=["\'](/storage/[^"\']+\.pdf)["\'][^>]*>',
                        r'<a[^>]+href=["\'](/papers/[^"\']+\.pdf)["\'][^>]*>',
                        r'<a[^>]+href=["\'](/pdf/[^"\']+\.pdf)["\'][^>]*>',
                        r'<a[^>]+href=["\'](https?://[^"\']+\.pdf)["\'][^>]*>',
                        ]

                for pattern in download_patterns:
                    match = re.search(pattern, html_text, re.IGNORECASE)
                    if match:
                        pdf_url = match.group(1)
                        print(f"  üìÑ PDF trouv√© via lien: {pdf_url}")
                        break

            # 3. Chercher dans les scripts JavaScript
            if not pdf_url:
                script_patterns = [
                        r'["\'](/storage/[^"\']+\.pdf)["\']',
                        r'["\'](/downloads/[^"\']+\.pdf)["\']',
                        r'["\'](/papers/[^"\']+\.pdf)["\']',
                        r'["\'](https?://[^"\']+\.pdf)["\']',
                        ]

                for pattern in script_patterns:
                    matches = re.findall(pattern, html_text, re.IGNORECASE)
                    for match in matches:
                        if '{pdf}' not in match and '{doi}' not in match:
                            pdf_url = match
                            print(f"  üìÑ PDF trouv√© via script: {pdf_url}")
                            break
                    if pdf_url:
                        break

            if not pdf_url:
                print(f"  ‚ùå Aucune URL PDF trouv√©e dans la page")
                continue

            # Construire l'URL compl√®te si relative
            if pdf_url.startswith('//'):
                pdf_url = 'https:' + pdf_url
            elif pdf_url.startswith('/'):
                pdf_url = f'https://{domain}{pdf_url}'
            elif not pdf_url.startswith('http'):
                pdf_url = urljoin(f'https://{domain}', pdf_url)

            print(f"  üîó URL PDF compl√®te: {pdf_url}")

            # Extraire le titre pour l'affichage seulement (pas pour le nom de fichier)
            title = None

            # Chercher dans les meta tags
            title_match = re.search(r'<meta[^>]+name=["\']citation_title["\'][^>]+content=["\']([^"\']+)["\']', html_text)
            if title_match:
                title = title_match.group(1)
                print(f"  üìù Titre trouv√©: {title[:80]}...")
            else:
                # Chercher dans le title tag
                title_match = re.search(r'<title>(.*?)</title>', html_text, re.IGNORECASE)
                if title_match:
                    title = title_match.group(1)
                    # Nettoyer
                    title = re.sub(r'^Sci-Hub\s*[|:]\s*', '', title)
                    title = re.sub(r'\s*-\s*DOI:\s*[^\s]+$', '', title)
                    print(f"  üìù Titre extrait: {title[:80]}...")

            # Utiliser uniquement le DOI sanitiz√© pour le nom de fichier
            filename = sanitize_doi_for_filename(doi)
            filepath = os.path.join(output_dir, filename)

            # V√©rifier si le fichier existe d√©j√†
            if os.path.exists(filepath):
                file_size = os.path.getsize(filepath) / 1024 / 1024
                print(f"  ‚è≠Ô∏è  Fichier existe d√©j√†: {filename} ({file_size:.1f} MB)")
                # Mettre quand m√™me √† jour le Parquet si le fichier existe d√©j√†
                update_parquet_status(doi)
                return filepath

            # Cr√©er le r√©pertoire si n√©cessaire
            os.makedirs(output_dir, exist_ok=True)

            # T√©l√©charger le PDF
            print(f"  ‚¨áÔ∏è  T√©l√©chargement vers: {filename}")

            pdf_response = requests.get(pdf_url, headers=headers, stream=True, timeout=60)

            if pdf_response.status_code != 200:
                print(f"  ‚ùå Erreur HTTP {pdf_response.status_code} lors du t√©l√©chargement")
                continue

            # V√©rifier que c'est bien un PDF
            content_type = pdf_response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and 'application/pdf' not in content_type:
                print(f"  ‚ö†Ô∏è  Content-Type suspect: {content_type}")
                # On continue quand m√™me

            # T√©l√©charger avec progression
            total_size = int(pdf_response.headers.get('content-length', 0))
            downloaded = 0

            with open(filepath, 'wb') as f:
                for chunk in pdf_response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            sys.stdout.write(f"\r  üì• Progression: {percent:.1f}%")
                            sys.stdout.flush()

            print()  # Nouvelle ligne

            # V√©rifier que le fichier est un PDF
            try:
                with open(filepath, 'rb') as f:
                    header = f.read(5)
                    if header[:4] != b'%PDF' and header != b'%PDF-':
                        # Essayer de chercher %PDF plus loin dans le fichier
                        f.seek(0)
                        content = f.read(500)
                        if b'%PDF' not in content:
                            os.remove(filepath)
                            print(f"  ‚ùå Le fichier t√©l√©charg√© n'est pas un PDF valide")
                            continue
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Erreur de v√©rification PDF: {e}")

            file_size = os.path.getsize(filepath) / 1024 / 1024
            print(f"  ‚úÖ T√©l√©chargement r√©ussi! ({file_size:.1f} MB)")

            # Mettre √† jour le Parquet apr√®s un t√©l√©chargement r√©ussi
            update_parquet_status(doi)

            return filepath

        except requests.exceptions.Timeout:
            print(f"  ‚è±Ô∏è  Timeout avec {domain}")
        except requests.exceptions.ConnectionError:
            print(f"  üîå Erreur de connexion avec {domain}")
        except Exception as e:
            print(f"  ‚ùå Erreur avec {domain}: {str(e)}")

    return None


def main():
    df = pd.read_parquet(PARQUET)
    # Convertir Publication_Year en num√©rique, les erreurs deviennent NaN
    df['Publication_Year'] = pd.to_numeric(df['Publication_Year'], errors='coerce')

    filtered_df = df[
            (df['Publication_Year'] < 2020) &
            (df['Is_Open_Access'] == True) &
            (df['PDF_on_S3'] == False)
            ]

    print("üìä Statistiques initiales:")
    filtered_df.info()

    # Compter combien d'articles seront t√©l√©charg√©s
    initial_count = len(filtered_df)
    print(f"\nüìä Nombre d'articles √† t√©l√©charger: {initial_count}")

    # Extraire tous les DOIs
    dois = filtered_df['DOI'].dropna()  # Supprime les NaN
    dois = dois[dois != '']             # Supprime les cha√Ænes vides
    dois = dois.tolist()                # Convertit en liste

    if not dois:
        print("‚ùå Aucun DOI valide √† traiter.")
        return

    successful_downloads = 0
    failed_downloads = 0

    for i, doi in enumerate(dois, 1):
        print("=" * 60)
        print(f"üìö Article {i}/{len(dois)}")
        print(f"üîç Recherche de l'article: {doi}")
        print("=" * 60)

        filepath = download_scihub_article(doi)

        if filepath:
            # Afficher le nom de fichier selon la convention
            filename = sanitize_doi_for_filename(doi)
            print(f"‚úÖ Article t√©l√©charg√© avec succ√®s!")
            print(f"üìÅ Nom du fichier (convention DOI): {filename}")
            print(f"üìÇ Chemin complet: {os.path.abspath(filepath)}")
            successful_downloads += 1
        else:
            print(f"\n‚ùå √âchec du t√©l√©chargement {doi}")
            failed_downloads += 1

        print("_" * 60)

        # Afficher les statistiques actuelles
        print(f"\nüìà Progression: {successful_downloads} r√©ussis, {failed_downloads} √©checs, {len(dois) - i} restants")

        # Petite pause entre les essais
        if i < len(dois):  # Pas de pause apr√®s le dernier
            print(f"‚è≥ Pause de {SLEEP} secondes...")
            time.sleep(SLEEP)

    # R√©sum√© final
    print("\n" + "="*60)
    print("üìä R√âSUM√â FINAL")
    print("="*60)
    print(f"‚úÖ T√©l√©chargements r√©ussis: {successful_downloads}")
    print(f"‚ùå T√©l√©chargements √©chou√©s: {failed_downloads}")
    print(f"üìÇ Total d'articles trait√©s: {len(dois)}")

    # V√©rifier combien d'articles restent √† t√©l√©charger
    df_updated = pd.read_parquet(PARQUET)
    df_updated['Publication_Year'] = pd.to_numeric(df_updated['Publication_Year'], errors='coerce')

    remaining_df = df_updated[
            (df_updated['Publication_Year'] < 2020) &
            (df_updated['Is_Open_Access'] == True) &
            (df_updated['PDF_on_S3'] == False)
            ]

    print(f"üìã Articles restant √† t√©l√©charger: {len(remaining_df)}")
    print("="*60)

if __name__ == "__main__":
    main()
